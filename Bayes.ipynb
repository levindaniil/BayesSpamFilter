{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем необходимые инструменты \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод для считывания файла и преобразование в список \n",
    "def ReadFile(path):\n",
    "\n",
    "    f = open(path)\n",
    "    raw = []\n",
    "    for line in f:    \n",
    "        raw.append(line)\n",
    "    f.close()\n",
    "    return raw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем класс Байесовского классификатора для определения спама\n",
    "class BayesClassifier():\n",
    "    \n",
    "    # конструктор класса\n",
    "    def __init__(self):\n",
    "        self.wordDict = {}\n",
    "\n",
    "    # метод для преобразования строк в массивы\n",
    "    def TokenizeText(self, raw):\n",
    "        # импортируем инструмент для преобразования в массивы с удалением знаков пунктуации\n",
    "        from nltk.tokenize import RegexpTokenizer\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        data = []\n",
    "        for r in raw:\n",
    "            words = (np.array(tokenizer.tokenize(r))) # токенизируем предложения\n",
    "            words = [w.lower() for w in words] # приводим слова к нижнему регистру\n",
    "            data.append(words)\n",
    "\n",
    "        return np.array(data)\n",
    "\n",
    "    # метод для разделения данных на массивы признаков (слова в предложении) и ответы (спам или нет) \n",
    "    def SplitData(self, data):\n",
    "        # создаем списки для спам- и хам-предложений\n",
    "        spam_data = []\n",
    "        ham_data = []\n",
    "\n",
    "        for s in data:\n",
    "            if s[0]=='ham':\n",
    "                ham_data.append(s[1:]) # если сообщение не является спамом\n",
    "            else:\n",
    "                spam_data.append(s[1:]) # если сообщение является спамом\n",
    "\n",
    "\n",
    "        return ham_data, spam_data\n",
    "\n",
    "    # метод для удаления стоп-слов и отдельных букв\n",
    "    def RemoveJunk(self, text):\n",
    "        # устанавливаем необходимый инструментарий для удаления стоп-слов\n",
    "        import nltk\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "        from nltk.corpus import stopwords\n",
    "        \n",
    "        # импортируем английские стоп-слова и добавляем к ним отдельные буквы ввиду неинформативности\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words = stop_words.union(('q','w','e','r','t','y','u','i','o','p','a','s','d',\n",
    "                                       'f','g','h','j','k','l','z','x','c','v','b','n','m'))\n",
    "        \n",
    "        # удаляем из каждого предложения стоп-слова\n",
    "        for i in range(len(text)):\n",
    "            text[i] = [w for w in text[i] if (not w in stop_words) and w.isalpha()]\n",
    "\n",
    "        return text\n",
    "\n",
    "    # метод для подготовки данных из файла\n",
    "    def PrepareData(self, raw):\n",
    "        data = self.TokenizeText(raw)\n",
    "        ham_data, spam_data = self.SplitData(data)\n",
    "        ham_data, spam_data = self.RemoveJunk(ham_data), self.RemoveJunk(spam_data)\n",
    "        return np.array(ham_data), np.array(spam_data)\n",
    "\n",
    "    # метод для формирования словаря \"спамовости\" слов\n",
    "    def CreateDict(self, ham_data, spam_data): \n",
    "        wordDict = {}\n",
    "        \n",
    "        # создаем соответствующие ключи для каждого слова \n",
    "        for s in spam_data:\n",
    "            for w in s:\n",
    "                wordDict[w]=0\n",
    "\n",
    "        for s in ham_data:\n",
    "            for w in s:\n",
    "                wordDict[w]=0\n",
    "        \n",
    "        # считаем количество спам- и хам-предложений\n",
    "        spam_count = len(spam_data)\n",
    "        ham_count = len(ham_data)   \n",
    "        \n",
    "        # для каждого слова рассчитываем \"спамовость\"\n",
    "        for w in wordDict:  \n",
    "            \n",
    "            # считаем частоту появления в спам- и хам- предложениях\n",
    "            spam_freq = 0\n",
    "            ham_freq = 0\n",
    "            \n",
    "            for s in spam_data:\n",
    "                spam_freq += (w in s)\n",
    "\n",
    "            for s in ham_data:\n",
    "                ham_freq += (w in s)\n",
    "            \n",
    "            # рассчитываем \"спамовость\" в соответствии с формулой Байеса\n",
    "            res_prob = spam_freq/(spam_freq+ham_freq)\n",
    "            \n",
    "            # заменяем крайние случаи (0/1) на близкие значения\n",
    "            if res_prob == 1:\n",
    "                res_prob = 0.99\n",
    "            elif res_prob == 0:\n",
    "                res_prob = 0.01\n",
    "            wordDict[w] = res_prob\n",
    "        \n",
    "        return wordDict\n",
    "\n",
    "    # метод для определения спама в массиве предложений\n",
    "    def IdentifySpam(self, data):\n",
    "        res_list = []\n",
    "        \n",
    "        # фиксируем обученный словарь \"спамовости\" слов\n",
    "        wordDict = self.wordDict\n",
    "        \n",
    "        # создаем списки со \"спамовостью\" для всех входящих в предложение слов\n",
    "        for newMes in data:\n",
    "            probs = []\n",
    "            for w in newMes:\n",
    "                if w in wordDict:\n",
    "                    probs.append(wordDict[w])\n",
    "            \n",
    "            # находим вероятности принадлежности к ham для расчета условной вероятности по Байесу\n",
    "            probs_rev = [(1-p) for p in probs]\n",
    "\n",
    "            top = np.prod(probs)\n",
    "            bottom = np.prod(probs_rev)\n",
    "            \n",
    "            # находим вероятность принадлежности сообщения к спаму\n",
    "            final_prob = top/(top+bottom)\n",
    "                        \n",
    "            res_list.append(final_prob)\n",
    "\n",
    "        return res_list\n",
    "\n",
    "    # итоговый метод для обучение классификатора\n",
    "    def fit(self, x_train):\n",
    "        ham_data, spam_data = self.PrepareData(x_train)\n",
    "        self.wordDict = self.CreateDict(ham_data, spam_data)        \n",
    "\n",
    "    # итоговый метод для предсказания ответов для массива сообщений\n",
    "    def predict(self, x_test):    \n",
    "        data = self.TokenizeText(x_test)\n",
    "        data = self.RemoveJunk(data)\n",
    "        res_list = self.IdentifySpam(data) \n",
    "        return np.array(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# создаем экземпляр классификатора\n",
    "Bayes = BayesClassifier()\n",
    "\n",
    "data = ReadFile('spam.txt')\n",
    "\n",
    "#разделяем данные в отношении 9 к 1 (на обучающую и тестовую выборки)\n",
    "data_train, data_test = tts(data, train_size = 0.9, test_size = 0.1, random_state=5).copy()\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "#токенизируем текст\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data_test_token = Bayes.TokenizeText(data_test)\n",
    "\n",
    "# сохраняем сообщения, удаляя слова ham/spam в начале и оставляя их в формате строк\n",
    "for s in (data_test):\n",
    "    if s[0]=='h':\n",
    "        X_test.append(s[3:])\n",
    "    else:\n",
    "        X_test.append(s[4:])\n",
    "    \n",
    "# фиксируем соответствующие ответы для последующей проверки качества\n",
    "for s in (data_test_token):\n",
    "    if s[0]=='ham':\n",
    "        y_test.append(0)\n",
    "    else:\n",
    "        y_test.append(1)\n",
    "        \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\levin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\levin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\levin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "AUC-ROC:  0.949677531307\n",
      "Accuracy: 0.985663082437\n"
     ]
    }
   ],
   "source": [
    "# обучаем классификатор\n",
    "Bayes.fit(data_train)\n",
    "\n",
    "# предсказываем ответы для отложенной выборки\n",
    "res = Bayes.predict(X_test)\n",
    "\n",
    "# сравниваем полученные ответы с изначальными и определяем точность работы классификатора с помощью различных метрик\n",
    "from sklearn import metrics \n",
    "\n",
    "AUC_ROC = metrics.roc_auc_score(y_test, res)\n",
    "print('\\nAUC-ROC: ', str(AUC_ROC))\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, res>0.5) #значения вероятности больше 0.5 относим к \"1\", меньше - к \"0\"\n",
    "print('Accuracy:', str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
